{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ETRI datasets we only use one body dataset.\n",
    "1. delete the dataset that body more than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define the folder containing the files and the destination folder\n",
    "input_folder = \"csv data\"\n",
    "output_folder = \"2bodynum\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def process_file(file_name):\n",
    "    if file_name.endswith(\".csv\"):  # Ensure it's a CSV file\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the 'trackingID' column exists and has more than one unique value\n",
    "        if 'trackingID' in df.columns and df['trackingID'].nunique() > 1:\n",
    "            # Construct the new file path in the output folder\n",
    "            new_file_path = os.path.join(output_folder, file_name)\n",
    "            \n",
    "            # Move the file to the output folder\n",
    "            shutil.move(file_path, new_file_path)\n",
    "            \n",
    "            print(f\"Moved: {file_path} to {new_file_path}\")\n",
    "\n",
    "# Get list of files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Use ThreadPoolExecutor to process files concurrently\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_file, file_list)\n",
    "\n",
    "print(\"Finished moving files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "input_folder = \"csv data\"\n",
    "output_folder = \"skeletons\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get the set of already processed filenames in the output directory\n",
    "processed_files = {os.path.splitext(f)[0] for f in os.listdir(output_folder) if f.endswith(\".skeleton\")}\n",
    "\n",
    "# Function to process a single CSV file\n",
    "def process_csv(file_path, output_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"Warning: File '{os.path.basename(file_path)}' is empty. Skipped processing.\")\n",
    "            return\n",
    "\n",
    "        # Extract joint data columns\n",
    "        joint_columns = ['3dX', '3dY', '3dZ', 'depthX', 'depthY', 'orientationW', 'orientationX', 'orientationY', 'orientationZ', 'trackingState']\n",
    "        missing_columns = [f'joint{i}_{coord}' for coord in joint_columns for i in range(1, 26) if f'joint{i}_{coord}' not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Warning: File '{os.path.basename(file_path)}' is missing columns: {missing_columns}. Skipped processing.\")\n",
    "            return\n",
    "        \n",
    "        # Initialize numpy arrays to store the data\n",
    "        joint_data = np.empty((len(df) * 25, len(joint_columns)))\n",
    "        for idx, coord in enumerate(joint_columns):\n",
    "            columns = [f'joint{i}_{coord}' for i in range(1, 26)]\n",
    "            joint_data[:, idx] = df[columns].values.ravel()\n",
    "        \n",
    "        # Create the combined DataFrame\n",
    "        combined_df = pd.DataFrame(joint_data, columns=[f'{coord}_Value' for coord in joint_columns])\n",
    "        combined_df['colorX'] = 0\n",
    "        combined_df['colorY'] = 0\n",
    "        combined_df['frameNumber'] = df['frameNum'].max() - 1 if 'frameNum' in df.columns else 0\n",
    "        combined_df['bodyindexID'] = df['bodyindexID'].unique()[0] if 'bodyindexID' in df.columns else 0\n",
    "        combined_df['num_joints'] = 25\n",
    "        combined_df['bodies_num'] = 1\n",
    "        combined_df['bodyID'] = 0\n",
    "        combined_df['clipedEdges'] = 0\n",
    "        combined_df['handLeftConfidence'] = 0\n",
    "        combined_df['handLeftState'] = 0\n",
    "        combined_df['handRightConfidence'] = 0\n",
    "        combined_df['handRightState'] = 0\n",
    "        combined_df['isRestricted'] = 0\n",
    "        combined_df['leanX'] = 0\n",
    "        combined_df['leanY'] = 0\n",
    "\n",
    "        if combined_df.empty:\n",
    "            print(f\"Warning: File '{os.path.basename(file_path)}' has no valid data after processing. Skipped.\")\n",
    "            return\n",
    "\n",
    "        lines = []\n",
    "\n",
    "        # Prepare data to be written to the file\n",
    "        lines.append(f\"{combined_df['frameNumber'].iloc[0]}\\n\")\n",
    "        lines.append(f\"{combined_df['bodies_num'].iloc[0]}\\n\")\n",
    "        \n",
    "        body_info = combined_df.iloc[0][['bodyID', 'clipedEdges', 'handLeftConfidence', 'handLeftState',\n",
    "                                         'handRightConfidence', 'handRightState', 'isRestricted', 'leanX',\n",
    "                                         'leanY']]\n",
    "        lines.append(' '.join(map(str, body_info)) + f\" {int(combined_df['trackingState_Value'].iloc[0])}\\n\")\n",
    "        lines.append(f\"{combined_df['num_joints'].iloc[0]}\\n\")\n",
    "\n",
    "        for i in range(len(combined_df)):\n",
    "            joint_data = combined_df.iloc[i][['3dX_Value', '3dY_Value', '3dZ_Value',\n",
    "                                              'depthX_Value', 'depthY_Value', 'colorX', 'colorY',\n",
    "                                              'orientationW_Value', 'orientationX_Value', 'orientationY_Value',\n",
    "                                              'orientationZ_Value']]\n",
    "            lines.append(' '.join(map(str, joint_data)) + f\" {int(combined_df['trackingState_Value'].iloc[i])}\\n\")\n",
    "\n",
    "            if (i + 1) % 25 == 0 and (i + 1) < len(combined_df):\n",
    "                lines.append(f\"{combined_df['bodies_num'].iloc[0]}\\n\")\n",
    "                body_info = combined_df.iloc[i][['bodyID', 'clipedEdges', 'handLeftConfidence', 'handLeftState',\n",
    "                                                 'handRightConfidence', 'handRightState', 'isRestricted', 'leanX',\n",
    "                                                 'leanY']]\n",
    "                lines.append(' '.join(map(str, body_info)) + f\" {int(combined_df['trackingState_Value'].iloc[i])}\\n\")\n",
    "                lines.append(f\"{combined_df['num_joints'].iloc[0]}\\n\")\n",
    "\n",
    "        # Write all lines to the file at once\n",
    "        with open(output_path, \"w\") as txt_file:\n",
    "            txt_file.writelines(lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{file_path}': {e}\")\n",
    "\n",
    "# Function to handle the processing of each file\n",
    "def handle_file(file_name):\n",
    "    if file_name.endswith(\".csv\"):  # Ensure it's a CSV file\n",
    "        # Extract relevant parts of the file name\n",
    "        parts = file_name.split('_')\n",
    "        if len(parts) < 4:\n",
    "            print(f\"Warning: File name '{file_name}' does not match expected pattern. Skipped.\")\n",
    "            return\n",
    "        \n",
    "        label = parts[0]  # Extract the label\n",
    "        person = parts[1]  # Extract the person ID\n",
    "        gesture = parts[2]  # Extract the gesture ID\n",
    "        camera = parts[3].split('.')[0]  # Extract the camera ID and remove the \".csv\" extension\n",
    "        \n",
    "        # Construct the output file name\n",
    "        output_file_name = f\"{label}{person}{gesture}{camera}.skeleton\"\n",
    "\n",
    "        # Check if the file has already been processed\n",
    "        if output_file_name in processed_files:\n",
    "            print(f\"File '{output_file_name}' already processed. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # Construct the full file path for input and output\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "        \n",
    "        # Process the CSV file\n",
    "        process_csv(input_file_path, output_file_path)\n",
    "\n",
    "# Get list of files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Use ThreadPoolExecutor to process files concurrently\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(handle_file, file_list)\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upated statistics folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "camera.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_folder = \"nturgb+d_skeletons\"\n",
    "etri_files = sorted(os.listdir(input_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the number after \"C\" from the filename\n",
    "def extract_number_after_C(filename):\n",
    "    # Find the part that contains 'C'\n",
    "    if 'C' in filename:\n",
    "        # Extract the number after 'C'\n",
    "        return int(filename.split('C')[1].split('.')[0])\n",
    "    return None\n",
    "\n",
    "# Extract numbers after \"C\" from each file name and maintain the order\n",
    "camera_numbers = [extract_number_after_C(file_name) for file_name in etri_files]\n",
    "\n",
    "# Save the numbers to a file named 'camera.txt'\n",
    "with open('camera.txt', 'w') as file:\n",
    "    for number in camera_numbers:\n",
    "        if number is not None:  # Ensure that we have a valid number\n",
    "            file.write(f\"{number}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A001P001G001C001.skeleton'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etri_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the number after 'A' and before 'P'\n",
    "def extract_label(filename):\n",
    "    # Extract the part after 'A' and before 'P'\n",
    "    start = filename.find('A') + 1\n",
    "    end = filename.find('P')\n",
    "    if start != -1 and end != -1:\n",
    "        return int(filename[start:end])\n",
    "    return None\n",
    "\n",
    "# Extract the numbers from each file name\n",
    "labels = [extract_label(file_name) for file_name in etri_files]\n",
    "\n",
    "# Pair the labels with their original filenames and sort by filenames\n",
    "sorted_labels = [label for _, label in sorted(zip(etri_files, labels))]\n",
    "\n",
    "# Save the sorted numbers to a file named 'label.txt'\n",
    "with open('label.txt', 'w') as file:\n",
    "    for label in sorted_labels:\n",
    "        if label is not None:  # Ensure that we have a valid label\n",
    "            file.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performer.tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the number after 'A' and before 'P'\n",
    "def extract_label(filename):\n",
    "    # Extract the part after 'A' and before 'P'\n",
    "    start = filename.find('P') + 1\n",
    "    end = filename.find('G')\n",
    "    if start != -1 and end != -1:\n",
    "        return int(filename[start:end])\n",
    "    return None\n",
    "\n",
    "# Extract the numbers from each file name\n",
    "labels = [extract_label(file_name) for file_name in etri_files]\n",
    "\n",
    "# Pair the labels with their original filenames and sort by filenames\n",
    "sorted_labels = [label for _, label in sorted(zip(etri_files, labels))]\n",
    "\n",
    "# Save the sorted numbers to a file named 'label.txt'\n",
    "with open('performer.txt', 'w') as file:\n",
    "    for label in sorted_labels:\n",
    "        if label is not None:  # Ensure that we have a valid label\n",
    "            file.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistic/replication.txt (value is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the number after 'A' and before 'P'\n",
    "def extract_label(filename):\n",
    "    # Extract the part after 'A' and before 'P'\n",
    "    start = filename.find('A') + 1\n",
    "    end = filename.find('P')\n",
    "    if start != -1 and end != -1:\n",
    "        return int(filename[start:end])\n",
    "    return None\n",
    "\n",
    "# Extract the numbers from each file name\n",
    "labels = [extract_label(file_name) for file_name in etri_files]\n",
    "\n",
    "# Save the sorted numbers to a file named 'label.txt'\n",
    "with open('label.txt', 'w') as label_file, open('replication.txt', 'w') as replication_file:\n",
    "    for label in labels:\n",
    "        if label is not None:  # Ensure that we have a valid label\n",
    "            label_file.write(f\"{label}\\n\")\n",
    "            replication_file.write(\"0\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the number after 'A' and before 'P'\n",
    "def extract_label(filename):\n",
    "    # Extract the part after 'A' and before 'P'\n",
    "    start = filename.find('G') + 1\n",
    "    end = filename.find('C')\n",
    "    if start != -1 and end != -1:\n",
    "        return int(filename[start:end])\n",
    "    return None\n",
    "\n",
    "# Extract the numbers from each file name\n",
    "labels = [extract_label(file_name) for file_name in etri_files]\n",
    "\n",
    "# Pair the labels with their original filenames and sort by filenames\n",
    "sorted_labels = [label for _, label in sorted(zip(etri_files, labels))]\n",
    "\n",
    "# Save the sorted numbers to a file named 'label.txt'\n",
    "with open('setup.txt', 'w') as file:\n",
    "    for label in sorted_labels:\n",
    "        if label is not None:  # Ensure that we have a valid label\n",
    "            file.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the number after 'A' and before 'P'\n",
    "def extract_label(filename):\n",
    "    # Extract the part after 'A' and before 'P'\n",
    "    start = filename.find('G') + 1\n",
    "    end = filename.find('C')\n",
    "    if start != -1 and end != -1:\n",
    "        return int(filename[start:end])\n",
    "    return None\n",
    "\n",
    "# Extract the numbers from each file name\n",
    "labels = [extract_label(file_name) for file_name in etri_files]\n",
    "\n",
    "# Pair the labels with their original filenames and sort by filenames\n",
    "sorted_labels = [label for _, label in sorted(zip(etri_files, labels))]\n",
    "\n",
    "# Save the sorted numbers to a file named 'label.txt'\n",
    "with open('setup.txt', 'w') as file:\n",
    "    for label in sorted_labels:\n",
    "        if label is not None:  # Ensure that we have a valid label\n",
    "            file.write(f\"{label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skes_available_name.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove underscores and the extension from each file name\n",
    "formatted_names = [file_name.replace('_', '').replace('.skeleton', '') for file_name in etri_files]\n",
    "\n",
    "# Save the formatted names to a file named 'formatted_names.txt'\n",
    "with open('skes_available_name.txt', 'w') as file:\n",
    "    for name in formatted_names:\n",
    "        file.write(name + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
