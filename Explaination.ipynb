{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import pickle\n",
    "from model import SGN\n",
    "from data import NTUDataLoaders, AverageMeter\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import h5py\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ntu120/X_full.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/ntu120/X_full.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     X \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Remove actions above 120\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carrt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ntu120/X_full.pkl'"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "with open('data/ntu120/X_full.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "# Remove actions above 120\n",
    "to_rem = []\n",
    "for file in X.keys():\n",
    "    if int(str(file).split('A')[1][:3]) > 60:\n",
    "        to_rem.append(file)\n",
    "for file in to_rem:\n",
    "    del X[file]\n",
    "\n",
    "def reshape_skeleton(skeleton):\n",
    "    \"\"\"\n",
    "    Reshape the skeleton data from shape [300, 150] to [20, 75].\n",
    "    \"\"\"\n",
    "    skeleton = skeleton[:20, :75]\n",
    "    return skeleton\n",
    "\n",
    "def predict_sgn(model, skeleton):\n",
    "    skeleton = torch.tensor(skeleton).cuda()\n",
    "    model.cuda()\n",
    "    out = model.eval_single(skeleton)\n",
    "    out = out.view((-1, skeleton.size(0)//skeleton.size(0), out.size(1)))\n",
    "    out = out.mean(1)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    out = np.argmax(out, axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "args = SimpleNamespace(batch_size=batch_size, train=0)\n",
    "\n",
    "sgn_ar = SGN(60, None, 20, args, 0).cuda()\n",
    "sgn_priv = SGN(40, None, 20, args, 0).cuda()\n",
    "sgn_ar.load_state_dict(torch.load('pretrained/action_60_sgnpt.pt')['state_dict'], strict=False)\n",
    "sgn_priv.load_state_dict(torch.load('pretrained/privacy_60_sgnpt.pt')['state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_joint_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_coord_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(frame, joint, coord)\n\u001b[0;32m      6\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m joint \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize LIME explainer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ar_explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(\n\u001b[1;32m---> 10\u001b[0m     training_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([reshape_skeleton(v)\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m()]),  \u001b[38;5;66;03m# Flattened training data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m     12\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAction_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m)],  \u001b[38;5;66;03m# Replace with actual class names if available\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     training_labels\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(k[\u001b[38;5;241m19\u001b[39m:\u001b[38;5;241m22\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mkeys()]),  \u001b[38;5;66;03m# Assuming action label is embedded in file name\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m ri_explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(\n\u001b[0;32m     18\u001b[0m     training_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([reshape_skeleton(v)\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mvalues()]),  \u001b[38;5;66;03m# Flattened training data\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     training_labels\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(k[\u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m12\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mkeys()]),  \u001b[38;5;66;03m# Assuming action label is embedded in file name\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import numpy as np\n",
    "\n",
    "feature_names = [\"frame_{}_joint_{}_coord_{}\".format(frame, joint, coord)\n",
    "                   for frame in range(20) for joint in range(25) for coord in ['x', 'y', 'z']]\n",
    "\n",
    "# Initialize LIME explainer\n",
    "ar_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array([reshape_skeleton(v).flatten() for v in X.values()]),  # Flattened training data\n",
    "    feature_names=feature_names,\n",
    "    mode='classification',\n",
    "    class_names=['Action_{}'.format(i) for i in range(60)],  # Replace with actual class names if available\n",
    "    training_labels=np.array([int(k[19:22]) - 1 for k in X.keys()]),  # Assuming action label is embedded in file name\n",
    ")\n",
    "\n",
    "ri_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array([reshape_skeleton(v).flatten() for v in X.values()]),  # Flattened training data\n",
    "    feature_names=feature_names,\n",
    "    mode='classification',\n",
    "    class_names=['Action_{}'.format(i) for i in range(40)],  # Replace with actual class names if available\n",
    "    training_labels=np.array([int(k[9:12]) - 1 for k in X.keys()]),  # Assuming action label is embedded in file name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "instance_limiter = 1\n",
    "\n",
    "# Sample a subset of instances for testing\n",
    "sampled_instances = random.sample(list(X.keys()), instance_limiter)\n",
    "\n",
    "# Storage for joint-wise importance scores\n",
    "joint_importances_ar = {joint: [] for joint in range(25)}  # For Action Recognition\n",
    "joint_importances_ri = {joint: [] for joint in range(25)}  # For Re-identification\n",
    "\n",
    "# Prediction functions for both models\n",
    "def predict_fn_ar(input_skeleton):\n",
    "    input_skeleton = input_skeleton.reshape(-1, 20, 75)\n",
    "    input_tensor = torch.tensor(input_skeleton, dtype=torch.float32).cuda()\n",
    "    with torch.no_grad():\n",
    "        output = sgn_ar.eval_single(input_tensor)\n",
    "        output = torch.softmax(output, 1).cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def predict_fn_ri(input_skeleton):\n",
    "    input_skeleton = input_skeleton.reshape(-1, 20, 75)\n",
    "    input_tensor = torch.tensor(input_skeleton, dtype=torch.float32).cuda()\n",
    "    with torch.no_grad():\n",
    "        output = sgn_priv.eval_single(input_tensor)\n",
    "        output = torch.softmax(output, 1).cpu().numpy()\n",
    "    return output\n",
    "\n",
    "# Iterate through the data and collect explanations\n",
    "for file_name in sampled_instances:\n",
    "    print('Gathering explanation for: ', file_name)\n",
    "    A = int(file_name[19:22]) - 1\n",
    "    P = int(file_name[9:12]) - 1\n",
    "    skeleton = X[file_name]\n",
    "\n",
    "    reshaped_skeleton = reshape_skeleton(skeleton)\n",
    "    flattened_skeleton = reshaped_skeleton.flatten()\n",
    "\n",
    "    # Get LIME explanation for Action Recognition\n",
    "    explanation_ar = ar_explainer.explain_instance(flattened_skeleton, predict_fn_ar, num_features=20*25*3, labels=[A])\n",
    "\n",
    "    # Get LIME explanation for Re-identification\n",
    "    explanation_ri = ri_explainer.explain_instance(flattened_skeleton, predict_fn_ri, num_features=20*25*3, labels=[P])\n",
    "\n",
    "    # Collect importance for each joint from both models\n",
    "    for feature_index, importance_value in explanation_ar.local_exp[A]:  # Assuming class 0\n",
    "        feature_name = ar_explainer.feature_names[feature_index]\n",
    "        # Extract joint number from feature name (e.g., \"frame_0_joint_7_coord_y\")\n",
    "        joint_num = int(feature_name.split('_')[3])\n",
    "        joint_importances_ar[joint_num].append(importance_value)\n",
    "\n",
    "    for feature_index, importance_value in explanation_ri.local_exp[P]:  # Assuming class 0\n",
    "        feature_name = ri_explainer.feature_names[feature_index]\n",
    "        joint_num = int(feature_name.split('_')[3])\n",
    "        joint_importances_ri[joint_num].append(importance_value)\n",
    "\n",
    "# Average the importance scores across sequences for each joint\n",
    "average_importances_ar = {joint: np.mean(importance) for joint, importance in joint_importances_ar.items()}\n",
    "average_importances_ri = {joint: np.mean(importance) for joint, importance in joint_importances_ri.items()}\n",
    "\n",
    "# Compare AR vs. RI to find joints with the specific patterns\n",
    "positive_ar_negative_ri = []\n",
    "negative_ar_positive_ri = []\n",
    "\n",
    "for joint in range(25):\n",
    "    if average_importances_ar[joint] > 0 and average_importances_ri[joint] < 0:\n",
    "        positive_ar_negative_ri.append(joint)\n",
    "    if average_importances_ar[joint] < 0 and average_importances_ri[joint] > 0:\n",
    "        negative_ar_positive_ri.append(joint)\n",
    "\n",
    "# Output the results\n",
    "print('Below are zero indexed (so add 1 when looking at the joint number):')\n",
    "print(\"Joints with positive importance for AR but negative for RI:\", positive_ar_negative_ri)\n",
    "print(\"Joints with negative importance for AR but positive for RI:\", negative_ar_positive_ri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
